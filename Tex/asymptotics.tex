\section{Randomised algorithm}
\emph{Uncertainty based on asymptotic likelihood and $\bar{w}$-distribution}\\
Let $\mathcal{L}_\infty$ be the log-likelihood function for a distribution, now let $\mathcal{L}_N$ denote the log-likelihood function based on $N$ observations from this distribution. Furthermore, let $N$ be a large number, for which $L_N \approx L_\infty$. 

\begin{equation}
\mathcal{L_N} = \frac{1}{N} \sum_{n=1}^N \ell_n \qquad \bar{w} \, s.t. \, \frac{\delta \mathcal{L}}{\delta \bar{w}} = \bar{0} \label{eq:ra1}
\end{equation}
Where $\ell_n$ is the log-likelihood of the $n^{th}$ observation. And $\bar{w}$ is the true weights for the distribution, then we combine the expressions from \eqref{eq:ra1}, such that for the true weights the following must be fulfilled:

\begin{equation}
\frac{1}{N} \sum_{n = 1}^N \frac{\delta \ell_n}{\delta \bar{w}} = 0 \label{eq:ra2}
\end{equation}

\emph{(Skal vi lige skrive lidt om at $\Delta w = w-w_0$ og er en lille forskydelse i vægtene? Eller er det en lille forskydelse?) }
For each of the $N$ observations, we can approximate the log-likelihood of the $n^{th}$ observation with this taylor expansion:

\begin{equation}
\ell_n(\bar{w}) = 
\ell_n\left(\bar{w}_0 \right) + 
\left. \frac{\delta \ell_n}{\delta \bar{w}} \right|_{\bar{w}_0} \Delta \bar{w} + 
\frac{1}{2} Tr \left[ \left. \frac{\delta \ell_n}{\delta \bar{w} \delta \bar{w}^T} \right|_{\bar{w}_0} \Delta \bar{w} \Delta \bar{w}^T \right] \label{eq:loglihood-oneObs}
\end{equation}
Or for the entire log-likelihood function: \textbf{Where did the trace go ?}
\begin{equation}
\mathcal{L}_N(\bar{w}) = 
\mathcal{L}_N\left(\bar{w}_0\right) + 
\left(\left.\frac{\delta \mathcal{L}_N}{\delta \bar{w}}\right|_{\bar{w}_0} \right)^T \cdot \Delta \bar{w} +
\frac{1}{2} \Delta \bar{w}^T \left( \left. \frac{\delta^2 \mathcal{L}_N}{\delta \bar{w} \delta \bar{w}^T}\right|_{\bar{w}_0} \right) \Delta \bar{w} + R \label{eq:entireLogLike}
\end{equation}
Where $R$ is the error of the approximation and assumed to be 0. Furthermore, we define $\bar{\bar{H}}_N = \left. \frac{\delta^2 \mathcal{L}_N}{\delta \bar{w} \delta \bar{w}^T}\right|_{\bar{w}_0}$, and $\bar{g}:N = \left.\frac{\delta \mathcal{L}_N}{\delta \bar{w}}\right|_{\bar{w}_0}$. And evaluate condition \eqref{eq:ra1} on $\bar{w}$:
\begin{equation}
\frac{\delta \mathcal{L_N}}{\delta \bar{w}} = \bar{g}_N+\bar{\bar{H}}_N \Delta \bar{w} = \bar{0}
\end{equation}
We replace $\Delta \bar{w}$ with $\hat{\Delta \bar{w}}$  as $N$ is a finite number, thus only approximating $\Delta \bar{w}$. Isolating $\hat{\Delta \bar{w}}$, and using Ljung \textbf{[REFERENCE?]}, we get: \textbf{Is this to soon to involve Ljung?}
\begin{equation}
\hat{\Delta \bar{w}} = - \bar{\bar{H}}_N^{-1} \cdot \bar{g}_N \overbrace{=}^{Ljung} -\bar{\bar{H}}
_0^{-1} \cdot \bar{g}_{\bar{w}} \left( \bar{w}_0 \right)
\end{equation}
\emph{(Forklaring af at $H_0$ er uafhængig af datasæt, mens $g$ nu er afhængig af $w$ evalueret i $w_0$)}
Besides getting an estimate for $\hat{\Delta \bar{w}}$, we can find the mean of the distribution:
\[
\left\langle \hat{\Delta \bar{w}} \right\rangle = - \bar{\bar{H}}_0^{-1} \bar{g}_0 = 0
\]
As $\delta \bar{w} = \bar{w}-\bar{w}_0$ ??mistet tråden?

\subsection{Covariance of $\bar{w}$ - distribution}


\textbf{Why do we do this???}

\begin{equation}
\left\langle \delta \bar{w} \delta \bar{w}^T \right\rangle_{N} = 
\left\langle \bar{\bar{H}}^{-1} \bar{g} \bar{g}^T \bar{\bar{H}}^{-1} \right\rangle
\overbrace{=}^{Ljung} \bar{\bar{H}}^{-1}_0 \left\langle \bar{g}\bar{g}^T \right\rangle \bar{\bar{H}}^{-1}_0 + R'
\end{equation}
With error $R' = O\left(\frac{1}{N}\right) \approx 0$, for large $N$. We look at the covariance of the gradient function
\begin{align}
\left\langle \bar{g} \bar{g}^T \right\rangle_N &= 
\frac{1}{N^2}\sum_{n, n' = 1}^N \left\langle \left. \frac{\delta \ell_n}{\delta_n \bar{w}} \right|_{\bar{w}_0} \left.\frac{\delta \ell_{n'}}{\delta_n \bar{w}} \right|_{\bar{w}_0} \right\rangle\\
 & = \frac{1}{N^2} \left(\sum_{n \neq n'} \underbrace{\left\langle \left. \frac{\delta \ell_n}{\delta \bar{w}} \right|_{\bar{w}_0} \right\rangle \cdot \left\langle \left. \frac{\delta \ell_{n'}}{\delta \bar{w}^T} \right|_{\bar{w}_0} \right\rangle }_0
 + \sum_{n =1 }^N \left\langle \left. \frac{\delta \ell_n}{\delta \bar{w}} \right|_{\bar{w}_0} \left. \frac{\delta \ell_n}{\delta \bar{w}^T} \right|_{\bar{w}_0}  \right\rangle \right)
\end{align}
Due to the assumption of independence, only the $N$ diagonal elements are non-zero. So;
\begin{equation}
\left\langle \bar{g} \bar{g}^T \right\rangle_N = \frac{1}{N} \left\langle \left. \frac{\delta \mathcal{L}}{\delta \bar{w}} \right|_{\bar{w}_0} \left. \frac{\delta \mathcal{L}}{\delta \bar{w}^T} \right|_{\bar{w}_0}  \right\rangle \label{eq:gg}
\end{equation}


\subsection{Proof that $\left\langle \left. \frac{\delta \mathcal{L}}{\delta \bar{w}} \right|_{\bar{w}_0} \left. \frac{\delta \mathcal{L}}{\delta \bar{w}^T} \right|_{\bar{w}_0}  \right\rangle = \bar{\bar{H}}_0$ }
Tekst test
\begin{equation}
\left\langle \bar{g} \bar{g}^T \right\rangle_N = 
\frac{1}{N^2} \sum_{n = 1}^N \int_{\Omega} 
\left. \frac{\delta \ell_n({\bar{x})}}{\delta \bar{w}} \right|_{\bar{w}_0}
\left. \frac{\delta \ell_n({\bar{x})}}{\delta \bar{w}} \right|_{\bar{w}_0}
p(\bar{x}) \delta x \label{eq:ggx}
\end{equation}
From \eqref{eq:gg} and \eqref{eq:ggx}, and setting $ \ell_n({\bar{x})} = p(\bar{x})$:
\begin{align}
\left.\bar{\bar{H}} \right|_{\bar{w}_0} &= 
\frac{1}{N} \sum_{n = 1}^N \int_{\Omega} 
\frac{\delta}{\delta \bar{w} \delta \bar{w}^T}
- \log p\left(\bar{x} \middle| \bar{w} \right) p(\bar{x}) \delta x\\
 &= \frac{1}{N} \sum_{n = 1}^N \int_{\Omega} - \frac{\delta}{\delta \bar{w}} \frac{1}{p\left(\bar{x}\right)}\frac{\delta }{\delta \bar{w}^T} p(\bar{x} | \bar{w}) p(\bar{x}) \delta \bar{x}\\
\end{align}
Now if $p(\bar{x}|\bar{w}_0) = p(x)$, then



\subsection{Uncertainty of prediction}
For a number of weight-vectors $\bar{w}$, we take the mean of predictions based on these weight-vectors;
\begin{equation}
\left\langle p \left(y \middle| \bar{x}, \bar{w} \right) \right\rangle \approx p \left(y \middle| \bar{x}, \hat{\bar{w}} \right) = p \left(y \middle| \bar{x}, \mathbf{E}(\bar{w}) \right)
\end{equation}
We now look at a small change in the prediction $\Delta p$, caused by a change of $\Delta \bar{w}$ in true weight vector $\bar{w}_0$. 
\begin{equation}
\Delta p = p \left(y \middle| \bar{x}, \bar{w}_0+\Delta \bar{w} \right)
- p \left(y \middle| \bar{x}, \bar{w}_0 \right) 
\approx \left.\frac{\delta p}{\delta \bar{w}} \right|_{w_0}\cdot \Delta \bar{w}
\end{equation}
The variance of $\Delta p$, can then be computed as
\begin{equation}
\left\langle (\Delta p)^2 \right\rangle = Tr\left[ 
\frac{\delta p}{\delta \bar{w}} \left(\frac{\delta p}{\delta \bar{w}}\right)^T
\left\langle \Delta \bar{w} \Delta \bar{W}^T \right\rangle \right]
= \frac{1}{N} \left(\frac{\delta p}{\delta \bar{w}}\right)^T 
 \bar{\bar{H}}^{-1} \frac{\delta p}{\delta \bar{w}}
\end{equation}

\subsubsection{For a linear model with known $\sigma^2$}
The prediction in a linear model is:
\begin{equation}
p(y|\bar{x},\bar{w}) = \frac{1}{\sqrt{2 \pi \sigma^2}} e^{\frac{(y-f(\bar{x|}|\bar{w})}{2 \sigma^2}} \label{linearprediction}
\end{equation}
Where $y$ is the target and $f(\bar{x|}|\bar{w})$ is the prediction. Differentiating \eqref{eq:linearprediction} with respect to $\bar{w}$: \emph{(Hvorfor er det vi gør det??)}
\begin{equation}
\frac{\delta p}{\delta w} = \frac{1}{\sqrt{2 \pi \sigma^2}} e^{-\frac{(y-f(\bar{x}|\bar{w})^2}{2\sigma^2}}-(y-f(\bar{x}|\bar{w}) \frac{\delta f(\bar{x}|\bar{w})}{\delta \bar{w}}
\end{equation}
We let $y = f(\bar{x}|\bar{w})+ \epsilon$. (Targets kan beskrives som en approximativ funktion + en fejl ..)
\begin{equation}
\frac{\delta p}{\delta \bar{w}} = \underbrace{ 
\frac{1}{\sqrt{2\pi \sigma^2}}
e^{-\frac{\epsilon^2}{2 \sigma^2}}\epsilon^2
}_{\texttt{const. w.r.t. } \bar{x} 
} 
\frac{\delta f(\bar{x}|\bar{w})}{\delta \bar{w}}^T \bar{\bar{H}}_0^{-1} \frac{\delta f(\bar{x}|\bar{w})}{\delta \bar{w}}
\end{equation}




