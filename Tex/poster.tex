\documentclass{beamer}

\usetheme[department=compute]{DTU}

\usepackage[orientation=landscape,size=a0,scale=1.8,debug]{beamerposter}             

\usepackage{multicol} % This is so we can have multiple columns of text side-by-side
\columnsep=100pt % This is the amount of white space between the columns in the poster
\columnseprule=3pt % This is the thickness of the black line between the columns in the poster


\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage{lmodern}
\usepackage[english]{babel}
%\usepackage{pgfplots}
%\pgfplotsset{compat=1.9}
%\usepackage{booktabs}
\usepackage{siunitx}


\title{Leverage based sampling for classification }
\author{Julian Kopka Larsen and Jesper Løve Hinrich}
\institute{DTU Compute}

\newcommand{\tabitem}{{\color{dtured}$\bullet$} }
\newenvironment{pblock}{\begin{minipage}[b]{\linewidth}
	\begin{block}}{\end{block} 	\end{minipage}\vspace*{15pt}}
\newcommand{\imblock}[1]{
\includegraphics[width=\linewidth]{#1}}

\setbeamerfont{frametitle}{size=\Huge}


\begin{document}
\frame{

\frametitle{Leverage based sampling for classification \\ \small{Jesper Hinrich Løve and Julian Kopka Larsen \\Supervised by Lars Kai Hansen}}

	
\begin{columns}[T]
    \begin{column}{.3\linewidth}

		\begin{block}{Abstract}
			We validate the results of leverage based sampling for LS-regression, shown by Ma et al. [1]. We explore the possibility of using the leverage based sampling distribution from LS-regression on 2 class classification, and introduce a %two
			 new approach for sampling from an leverage distribution (important points).
		\end{block}
			
		\begin{pblock}{Motivation}
			The importance of sampling methods is initiated by very large datasets where it is not feasible to use all of the available data. This is illustrated by the rise in online access to video data. These data contain many frames that are basically the same and therefore redundant. 
		\end{pblock}	
			

	\begin{pblock}{Research Questions}	
	\begin{itemize}
	\item Will the regression based sampling distribution improve our performance in classification?
	\item Can leverage based sampling be generalized to classification?
	\end{itemize}
	\end{pblock}

	\begin{pblock}{Leveraging in general}
	For any leveraging we want to find for each datapoint a leverage-score representing the importance of the point. This we can normalize into a distribution to sample from.
	\end{pblock}

	\begin{pblock}{Datasets}
	\includegraphics[width=\linewidth]{../Data_distributions.png}
	
	These datasets are drawn from distributions defined in Ma et al. [1] and have: 
	\begin{itemize}
	\item GA: Nearly uniform leverage-scores
	\item T3: Mildly non-uniform leverage-scores
	\item T1: Very non-uniform leverage-scores
	\end{itemize}  
	
	\end{pblock}
	
	
	\end{column}
	
	\begin{column}{.69\linewidth}
			\begin{block}{Process}
				\includegraphics[width=\linewidth]{ThoughModel.png}
			\end{block}
	
	
	\begin{columns}[t]
	
    	\begin{column}{.5\linewidth}


	\begin{pblock}{LS-based Distribution}	
	We seek to find the effect that a datapoint's class has on the predicted class for that datapoint.
	    \begin{equation}
	    \label{dyhatdy}
	    \frac{\delta \hat{y}_n}{\delta y_n}
	    \end{equation}
	There is a closed form solution which is linear in $y$
		\begin{equation*}
			\hat{\beta}_{OLS} = \left( X^T X \right)^{-1} X^T y \quad \text{where} \quad \hat{y}_n = X_n*\hat{\beta}
		\end{equation*}
		
	Therefore the leverage-score \eqref{dyhatdy} is the coefficient
		\begin{equation*}
			\frac{\delta \hat{y}_n}{\delta y_n} = X \left( X^T X \right)^{-1} X^T
		\end{equation*}
	
	\end{pblock}

	\begin{pblock}{Test results}
	We compared logistic regression when sampling from a LS-distribution (blue) vs. a uniform-distribution (red). The mean, 25th and 75th quantile are plotted.
	\includegraphics[width=.33\linewidth]{GA.png}	\includegraphics[width=.33\linewidth]{T3.png}	\includegraphics[width=.33\linewidth]{T1.png}\\
	It clearly shows that a LS-distribution sample scheme, does not outperform a uniform-distribution for classification. The results shown are for dimension $p = 10$ and $N = 1000$ datapoints, but it is consistent when varying $p$ and $N$. 

	\end{pblock}

	
	
    \end{column}
    \begin{column}{.5\linewidth}
    
    
    	\begin{pblock}{Sensitivity Based Distribution}
    	The target is again \eqref{dyhatdy} 
    	\begin{equation*}
    	\label{optimum}
    	 \hat{y}_n = p(y|\bar{x},\bar{w}) \quad \bar{w} \  \text{s.t.} \ \frac{\delta L}{\delta\bar{w}}=0
    	\end{equation*}
    	Since \ref{optimum} depends both directly and indirectly on $y$ we see that
    	\begin{eqnarray*}
    	&\frac{\delta}{\delta y} \frac{\delta L}{\delta w} = 0\\
    	\Downarrow & \\
    	&\frac{\delta^2 L}{\delta y \delta \bar{w}} + \frac{\delta^2 L}{\delta \bar{w} \delta \bar{w}^T} \frac{\delta \bar{w}}{\delta y}= 0
    	\end{eqnarray*}
    	
    	and from this we can get our leverage-score \eqref{dyhatdy}
    	
    	\begin{equation*}
    		\frac{\delta \hat{y}_n}{\delta y_n}=\frac{\delta p(y|\bar{x}_n,\bar{w})}{\delta \bar{w}^T} \frac{\delta \bar{w}}{\delta y} = - \frac{\delta p(y|\bar{x}_n,\bar{w})}{\delta \bar{w}^T} \left[ \frac{\delta^2 L}{\delta \bar{w} \delta \bar{w}^T} \right]^{-1} \frac{\delta^2 L}{\delta y \delta \bar{w}}
    	\end{equation*}
    	
    	When evaluating this model, weights trained on a small training-set is used. This is expected to be better than LS-based sampling since it introduces dependence on class information.
    	\end{pblock}
    	
    
   % \begin{pblock}{Uncertainty Based Distribution}
   % 	bla
   % \end{pblock}
    
    
    \begin{pblock}{Conclusion}
    The LS-based leverage sampling gives no advantage over uniform sampling and generally performs worse. LS-distribution is based on what is important for linear regression, it does not have an advantage in finding important points for classification.
    \end{pblock}
    			
    \begin{pblock}{References}
   		[1] Ma et. al. \emph{A Statistical Perspective on Algorithmic Leveraging} 2013
    \end{pblock}
    
    \end{column}
    \end{columns}
    \end{column}
  \end{columns}
	
}
\end{document}